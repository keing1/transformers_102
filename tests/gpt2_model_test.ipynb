{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I validate my implementation of transformer components and GPT-2 small by:\n",
    "- Comparing the behavior of a GPT-2 transformer block with my own implementation of a GPT-2 transformer block\n",
    "- Comparing the behavior of the GPT-2 embedding components with my own implementation of the GPT-2 embedding components\n",
    "- Comparing the behavior of the full GPT-2 model with my own implementation of the full GPT-2 model\n",
    "- Sampling from my implementation of the full GPT-2 model\n",
    "\n",
    "There are slight deviations between the outputs of my model and that of the original GPT-2 model. As shown below, this appears to be primarily due to the original GPT-2 model using a slightly different variant of GeLU from both me and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import sys\n",
    "import os\n",
    "notebook_path = os.path.abspath('')\n",
    "project_root = os.path.join(notebook_path, '..')\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing GPT2 transformer block (and components) with my implementation of a GPT2 transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "TransformerDecoderBlock(\n",
      "  (norm_layer1): LayerNorm()\n",
      "  (norm_layer2): LayerNorm()\n",
      "  (mha_block): MultiheadAttentionBlock(\n",
      "    (linear_q): Linear()\n",
      "    (linear_k): Linear()\n",
      "    (linear_v): Linear()\n",
      "    (linear_o): Linear()\n",
      "    (dropout_layer_1): Dropout()\n",
      "    (dropout_layer_2): Dropout()\n",
      "  )\n",
      "  (mlp_block): MLPBlock(\n",
      "    (linear1): Linear()\n",
      "    (linear2): Linear()\n",
      "    (dropout_layer): Dropout()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from src.blocks import TransformerDecoderBlock\n",
    "from src.models import GPT2SmallModel\n",
    "\n",
    "\n",
    "gpt2_block = model.h[0]\n",
    "my_block = GPT2SmallModel().transformer_blocks[0]\n",
    "#my_block = TransformerDecoderBlock(768, 12, 3072, 'gelu', 'layer_norm', 'mlpblock')\n",
    "\n",
    "my_block.norm_layer1.weight, my_block.norm_layer1.bias = gpt2_block.ln_1.weight, gpt2_block.ln_1.bias\n",
    "my_block.norm_layer2.weight, my_block.norm_layer2.bias = gpt2_block.ln_2.weight, gpt2_block.ln_2.bias\n",
    "\n",
    "my_block.mlp_block.linear1.weight, my_block.mlp_block.linear1.bias = t.nn.Parameter(gpt2_block.mlp.c_fc.weight.T), gpt2_block.mlp.c_fc.bias\n",
    "my_block.mlp_block.linear2.weight, my_block.mlp_block.linear2.bias = t.nn.Parameter(gpt2_block.mlp.c_proj.weight.T), gpt2_block.mlp.c_proj.bias\n",
    "\n",
    "(wq,wk,wv) = t.chunk(gpt2_block.attn.c_attn.weight.T, 3, dim=0)\n",
    "(bq,bk,bv) = t.chunk(gpt2_block.attn.c_attn.bias, 3, dim=0)\n",
    "\n",
    "my_block.mha_block.linear_q.weight, my_block.mha_block.linear_q.bias = t.nn.Parameter(wq), t.nn.Parameter(bq)\n",
    "my_block.mha_block.linear_k.weight, my_block.mha_block.linear_k.bias = t.nn.Parameter(wk), t.nn.Parameter(bk)\n",
    "my_block.mha_block.linear_v.weight, my_block.mha_block.linear_v.bias = t.nn.Parameter(wv), t.nn.Parameter(bv)\n",
    "my_block.mha_block.linear_o.weight, my_block.mha_block.linear_o.bias = t.nn.Parameter(gpt2_block.attn.c_proj.weight.T), gpt2_block.attn.c_proj.bias\n",
    "\n",
    "my_block.eval()\n",
    "gpt2_block.eval()\n",
    "\n",
    "print(gpt2_block)\n",
    "print(my_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing transformer blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 Transformer block\n",
      "tensor([[[  7.6788,  -9.8812,  -1.1139,  ...,   0.0982,   0.7792,   6.4225],\n",
      "         [ 14.1291,  -1.0713, -11.0099,  ...,  -5.2583,   3.5006,  -9.6592],\n",
      "         [  7.8788,  -7.2983,  -9.1949,  ...,  -1.1361,   9.0922,  -0.3065],\n",
      "         [ 11.2969,  -0.2765,  -1.4333,  ...,  28.7280,   0.3489,  21.2341],\n",
      "         [ -6.0503, -11.5091, -14.0023,  ...,   5.4135,   0.0551,  12.2189]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "My transformer block\n",
      "tensor([[[  7.6774,  -9.8810,  -1.1129,  ...,   0.0978,   0.7773,   6.4216],\n",
      "         [ 14.1287,  -1.0719, -11.0098,  ...,  -5.2597,   3.5024,  -9.6582],\n",
      "         [  7.8799,  -7.2980,  -9.1946,  ...,  -1.1365,   9.0916,  -0.3069],\n",
      "         [ 11.2966,  -0.2760,  -1.4336,  ...,  28.7270,   0.3493,  21.2344],\n",
      "         [ -6.0488, -11.5085, -14.0011,  ...,   5.4129,   0.0542,  12.2184]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = t.randn((1,5,768))\n",
    "\n",
    "seq_len = x.shape[-2]\n",
    "att_mask = t.where(t.arange(seq_len).unsqueeze(1) < t.arange(seq_len), -t.inf, 0)\n",
    "\n",
    "gpt2_block.eval()\n",
    "my_block.eval()\n",
    "\n",
    "print(\"GPT2 Transformer block\")\n",
    "print(gpt2_block(x)[0])\n",
    "print(\"My transformer block\")\n",
    "print(my_block(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization\n",
      "GPT2 Layer Norm 1\n",
      "tensor([[[-0.2543,  0.1051, -0.1672,  ..., -0.0284, -0.0908, -0.1497],\n",
      "         [ 0.0776,  0.0596,  0.1473,  ..., -0.0333,  0.2456, -0.0011],\n",
      "         [-0.2199, -0.0304,  0.0687,  ..., -0.1189,  0.2698,  0.2543],\n",
      "         [ 0.3683, -0.4476,  0.1157,  ...,  0.2192, -0.1883, -0.2082],\n",
      "         [-0.1518, -0.1926,  0.1469,  ...,  0.0816, -0.1222,  0.0852]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "My Layer Norm 1\n",
      "tensor([[[-0.2543,  0.1051, -0.1672,  ..., -0.0284, -0.0908, -0.1497],\n",
      "         [ 0.0776,  0.0596,  0.1473,  ..., -0.0333,  0.2456, -0.0011],\n",
      "         [-0.2199, -0.0304,  0.0687,  ..., -0.1189,  0.2698,  0.2543],\n",
      "         [ 0.3683, -0.4476,  0.1157,  ...,  0.2192, -0.1883, -0.2082],\n",
      "         [-0.1518, -0.1926,  0.1469,  ...,  0.0816, -0.1222,  0.0852]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "GPT2 Layer Norm 2\n",
      "tensor([[[-0.1046,  0.1222, -0.1344,  ..., -0.1356, -0.5330, -0.7598],\n",
      "         [ 0.0902,  0.0700,  0.2890,  ..., -0.1691,  1.8359,  0.0809],\n",
      "         [-0.0844, -0.0336,  0.1832,  ..., -0.7513,  2.0061,  1.5260],\n",
      "         [ 0.2607, -0.5136,  0.2465,  ...,  1.5497, -1.2195, -1.0912],\n",
      "         [-0.0445, -0.2202,  0.2886,  ...,  0.6128, -0.7545,  0.5694]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "My Layer Norm 2\n",
      "tensor([[[-0.1046,  0.1222, -0.1344,  ..., -0.1356, -0.5330, -0.7598],\n",
      "         [ 0.0902,  0.0700,  0.2890,  ..., -0.1691,  1.8359,  0.0809],\n",
      "         [-0.0844, -0.0336,  0.1832,  ..., -0.7513,  2.0061,  1.5260],\n",
      "         [ 0.2607, -0.5136,  0.2465,  ...,  1.5497, -1.2195, -1.0912],\n",
      "         [-0.0445, -0.2202,  0.2886,  ...,  0.6128, -0.7545,  0.5694]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalization\")\n",
    "\n",
    "print(\"GPT2 Layer Norm 1\")\n",
    "print(gpt2_block.ln_1(x))\n",
    "print(\"My Layer Norm 1\")\n",
    "print(my_block.norm_layer1(x))\n",
    "\n",
    "print(\"GPT2 Layer Norm 2\")\n",
    "print(gpt2_block.ln_2(x))\n",
    "print(\"My Layer Norm 2\")\n",
    "print(my_block.norm_layer2(x))\n",
    "\n",
    "print(t.allclose(my_block.norm_layer1(x), gpt2_block.ln_1(x), atol=1e-7))\n",
    "print(t.allclose(my_block.norm_layer2(x), gpt2_block.ln_2(x), atol=1e-7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head attention\n",
      "GPT2 Attention\n",
      "tensor([[[ -6.8100,   4.8123,  -0.3440,  ...,  -0.1354,  -1.1235,   0.2230],\n",
      "         [  2.7269,  -4.0638, -20.8347,  ...,  -0.1602,   0.6963,   1.3314],\n",
      "         [-10.2985,  -9.5725,   0.0964,  ...,  -0.5052,   0.9192,  -0.1420],\n",
      "         [ 11.7542,  -3.4020,   3.7599,  ...,  -1.3550,  -1.2914,   1.2410],\n",
      "         [ -2.9027,  -8.9946,   1.8246,  ...,  -1.0319,   0.9347,   0.3394]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "My attention\n",
      "tensor([[[ -6.8100,   4.8123,  -0.3440,  ...,  -0.1354,  -1.1235,   0.2230],\n",
      "         [  2.7269,  -4.0638, -20.8347,  ...,  -0.1602,   0.6963,   1.3314],\n",
      "         [-10.2985,  -9.5725,   0.0964,  ...,  -0.5052,   0.9192,  -0.1420],\n",
      "         [ 11.7542,  -3.4020,   3.7599,  ...,  -1.3550,  -1.2914,   1.2410],\n",
      "         [ -2.9027,  -8.9945,   1.8246,  ...,  -1.0319,   0.9347,   0.3394]]],\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Multi-head attention\")\n",
    "print(\"GPT2 Attention\")\n",
    "print(gpt2_block.attn(x)[0])\n",
    "print(\"My attention\")\n",
    "print(my_block.mha_block(x, attention_mask=att_mask))\n",
    "\n",
    "\n",
    "print(t.allclose(my_block.mha_block(x, attention_mask=att_mask), gpt2_block.attn(x)[0], atol=1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "GPT2 MLP\n",
      "tensor([[[ 14.2362, -19.7986,  -0.4560,  ...,   9.6376,  13.1956,  14.2020],\n",
      "         [ 14.4566,  -0.8697,  -4.6146,  ...,  -9.3027,  -1.3662, -13.2572],\n",
      "         [  8.8456, -10.3722,  -9.0671,  ...,  -3.6412,  15.9834,  -1.3890],\n",
      "         [  6.9620,  -2.3900,   0.5530,  ...,  45.6544,   9.9884,  23.9408],\n",
      "         [ -6.6012, -11.3722, -19.0957,  ...,  12.6699,   0.8963,  13.0032]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "My MLP\n",
      "tensor([[[ 14.2347, -19.7987,  -0.4551,  ...,   9.6366,  13.1943,  14.2017],\n",
      "         [ 14.4565,  -0.8689,  -4.6140,  ...,  -9.3020,  -1.3659, -13.2582],\n",
      "         [  8.8456, -10.3701,  -9.0677,  ...,  -3.6403,  15.9842,  -1.3896],\n",
      "         [  6.9620,  -2.3903,   0.5546,  ...,  45.6533,   9.9881,  23.9401],\n",
      "         [ -6.6014, -11.3713, -19.0937,  ...,  12.6704,   0.8955,  13.0023]]],\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"MLP\")\n",
    "print(\"GPT2 MLP\")\n",
    "print(gpt2_block.mlp(x))\n",
    "print(\"My MLP\")\n",
    "print(my_block.mlp_block(x))\n",
    "\n",
    "print(t.allclose(my_block.mlp_block(x), gpt2_block.mlp(x), atol=1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing GeLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeLU\n",
      "GPT2 GeLU\n",
      "tensor([-0.0454, -0.1560, -0.1437, -0.0401, -0.1681, -0.0629,  1.4251,  0.2870,\n",
      "         0.0314, -0.1547])\n",
      "My GeLU\n",
      "tensor([-0.0455, -0.1560, -0.1435, -0.0401, -0.1680, -0.0629,  1.4253,  0.2870,\n",
      "         0.0314, -0.1545])\n",
      "Torch GeLU\n",
      "tensor([-0.0455, -0.1560, -0.1435, -0.0401, -0.1680, -0.0629,  1.4253,  0.2870,\n",
      "         0.0314, -0.1545])\n"
     ]
    }
   ],
   "source": [
    "from src.activations import gelu\n",
    "from torch.nn.functional import gelu as torch_gelu\n",
    "\n",
    "print(\"GeLU\")\n",
    "x = t.randn((10,))\n",
    "\n",
    "print(\"GPT2 GeLU\")\n",
    "print(gpt2_block.mlp.act(x))\n",
    "print(\"My GeLU\")\n",
    "print(gelu(x))\n",
    "print(\"Torch GeLU\")\n",
    "print(torch_gelu(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing MLP using GPT2 GeLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My MLP with their GELU\n",
      "GPT2 MLP Block\n",
      "tensor([[[-34.8654,  -6.9767, -40.3958,  ...,  14.7084,  38.7968,   7.9544],\n",
      "         [ -3.4955,  -6.0676,  19.7404,  ...,  21.4285,   1.5369,   9.7805],\n",
      "         [ -0.1529,  -8.0439,  -7.7301,  ...,  -6.2543,   3.0814,   2.3790],\n",
      "         ...,\n",
      "         [ 17.7109,  -0.1405, -13.7513,  ...,   8.3319,  16.8486,  10.7691],\n",
      "         [ 19.4083, -11.3437, -18.7254,  ...,   3.2344,   2.6756,  38.0403],\n",
      "         [ -1.3154, -12.0614,  -5.8063,  ...,   0.9843,  -0.0852,  -6.5585]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "My MLP Block with GPT2 GeLU\n",
      "tensor([[[-34.8654,  -6.9767, -40.3958,  ...,  14.7084,  38.7968,   7.9544],\n",
      "         [ -3.4955,  -6.0676,  19.7404,  ...,  21.4285,   1.5369,   9.7805],\n",
      "         [ -0.1529,  -8.0439,  -7.7301,  ...,  -6.2543,   3.0814,   2.3790],\n",
      "         ...,\n",
      "         [ 17.7109,  -0.1405, -13.7513,  ...,   8.3319,  16.8486,  10.7691],\n",
      "         [ 19.4083, -11.3437, -18.7254,  ...,   3.2344,   2.6756,  38.0403],\n",
      "         [ -1.3154, -12.0614,  -5.8063,  ...,   0.9843,  -0.0852,  -6.5585]]],\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "True\n",
      "The main difference between our models is in the MLP block, because OpenAI used a slightly different version of GeLU.\n"
     ]
    }
   ],
   "source": [
    "print(\"My MLP with their GELU\")\n",
    "x = t.randn((1, 10, 768))\n",
    "\n",
    "print(\"GPT2 MLP Block\")\n",
    "gpt2_mlp = gpt2_block.mlp(x)\n",
    "print(gpt2_mlp)\n",
    "\n",
    "print(\"My MLP Block with GPT2 GeLU\")\n",
    "my_mlp = my_block.mlp_block.linear2(gpt2_block.mlp.act(my_block.mlp_block.linear1(x)))\n",
    "print(my_mlp)\n",
    "\n",
    "print(t.allclose(gpt2_mlp, my_mlp, atol=1e-5))\n",
    "\n",
    "print(\"The main difference between our models is in the MLP block, because OpenAI used a slightly different version of GeLU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing GPT2 embedding with my implementation of the GPT2 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1212,  318,  257, 1332, 6827,   13]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import GPT2SmallModel\n",
    "my_model = GPT2SmallModel()\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "my_model.embedding_layer.position_embedding_layer.weight = model.wpe.weight\n",
    "my_model.embedding_layer.token_embedding_layer.weight = model.wte.weight\n",
    "\n",
    "text = \"This is a test sentence.\"\n",
    "tokens = t.tensor(tokenizer(text)['input_ids']).unsqueeze(0)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding outputs\n",
      "GPT2 embedding output\n",
      "tensor([[[ 0.0065, -0.2930,  0.0762,  ...,  0.0184, -0.0275,  0.1638],\n",
      "         [ 0.0142, -0.0437, -0.0393,  ...,  0.1487, -0.0278, -0.0255],\n",
      "         [-0.0464, -0.0791,  0.1016,  ...,  0.0623,  0.0928, -0.0598],\n",
      "         [-0.0580,  0.0095,  0.2207,  ..., -0.0635,  0.0760, -0.0543],\n",
      "         [-0.0888, -0.0326,  0.1666,  ..., -0.2539, -0.0370, -0.2046],\n",
      "         [ 0.0562, -0.0452,  0.1596,  ..., -0.0676,  0.0567,  0.0888]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "My embedding output\n",
      "tensor([[[ 0.0065, -0.2930,  0.0762,  ...,  0.0184, -0.0275,  0.1638],\n",
      "         [ 0.0142, -0.0437, -0.0393,  ...,  0.1487, -0.0278, -0.0255],\n",
      "         [-0.0464, -0.0791,  0.1016,  ...,  0.0623,  0.0928, -0.0598],\n",
      "         [-0.0580,  0.0095,  0.2207,  ..., -0.0635,  0.0760, -0.0543],\n",
      "         [-0.0888, -0.0326,  0.1666,  ..., -0.2539, -0.0370, -0.2046],\n",
      "         [ 0.0562, -0.0452,  0.1596,  ..., -0.0676,  0.0567,  0.0888]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Embedding outputs\")\n",
    "print(\"GPT2 embedding output\")\n",
    "gpt2_emb_out = model.wte(tokens) + model.wpe(t.arange(tokens.shape[-1]))\n",
    "print(gpt2_emb_out)\n",
    "print(\"My embedding output\")\n",
    "my_emb_out = my_model.embedding_layer(tokens)\n",
    "print(my_emb_out)\n",
    "\n",
    "t.allclose(gpt2_emb_out, my_emb_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing GPT2 model vs. my implementation of the GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1212,  318,  257, 1332, 6827,   13]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import GPT2SmallModel\n",
    "my_model = GPT2SmallModel()\n",
    "my_model.eval()\n",
    "\n",
    "# Importing GPT2LMHeadModel instead of GPT2Model because it applies the final unembed to get logits\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "gpt2_model.eval()\n",
    "\n",
    "text = \"This is a test sentence.\"\n",
    "tokens = t.tensor(tokenizer(text)['input_ids']).unsqueeze(0)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying over weights\n",
    "my_model.embedding_layer.position_embedding_layer.weight = gpt2_model.transformer.wpe.weight\n",
    "my_model.embedding_layer.token_embedding_layer.weight = gpt2_model.transformer.wte.weight\n",
    "\n",
    "my_model.final_ln.weight, my_model.final_ln.bias = gpt2_model.transformer.ln_f.weight, gpt2_model.transformer.ln_f.bias\n",
    "\n",
    "my_model_block_list = my_model.transformer_blocks\n",
    "gpt2_model_block_list = gpt2_model.transformer.h\n",
    "\n",
    "for my_block, gpt2_block in zip(my_model_block_list, gpt2_model_block_list):\n",
    "    my_block.norm_layer1.weight, my_block.norm_layer1.bias = gpt2_block.ln_1.weight, gpt2_block.ln_1.bias\n",
    "    my_block.norm_layer2.weight, my_block.norm_layer2.bias = gpt2_block.ln_2.weight, gpt2_block.ln_2.bias\n",
    "\n",
    "    my_block.mlp_block.linear1.weight, my_block.mlp_block.linear1.bias = t.nn.Parameter(gpt2_block.mlp.c_fc.weight.T), gpt2_block.mlp.c_fc.bias\n",
    "    my_block.mlp_block.linear2.weight, my_block.mlp_block.linear2.bias = t.nn.Parameter(gpt2_block.mlp.c_proj.weight.T), gpt2_block.mlp.c_proj.bias\n",
    "\n",
    "    (wq,wk,wv) = t.chunk(gpt2_block.attn.c_attn.weight.T, 3, dim=0)\n",
    "    (bq,bk,bv) = t.chunk(gpt2_block.attn.c_attn.bias, 3, dim=0)\n",
    "\n",
    "    my_block.mha_block.linear_q.weight, my_block.mha_block.linear_q.bias = t.nn.Parameter(wq), t.nn.Parameter(bq)\n",
    "    my_block.mha_block.linear_k.weight, my_block.mha_block.linear_k.bias = t.nn.Parameter(wk), t.nn.Parameter(bk)\n",
    "    my_block.mha_block.linear_v.weight, my_block.mha_block.linear_v.bias = t.nn.Parameter(wv), t.nn.Parameter(bv)\n",
    "    my_block.mha_block.linear_o.weight, my_block.mha_block.linear_o.bias = t.nn.Parameter(gpt2_block.attn.c_proj.weight.T), gpt2_block.attn.c_proj.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logits\n",
      "GPT2 model logits\n",
      "tensor([[[ -35.8890,  -35.2049,  -39.1336,  ...,  -42.4869,  -41.8197,\n",
      "           -36.0383],\n",
      "         [-107.7291, -108.0175, -113.2967,  ..., -116.4645, -115.7443,\n",
      "          -110.8654],\n",
      "         [-111.7507, -111.5704, -114.5443,  ..., -120.7242, -117.1756,\n",
      "          -112.3996],\n",
      "         [ -86.1846,  -88.5057,  -94.3530,  ..., -101.3573,  -98.6974,\n",
      "           -91.1616],\n",
      "         [-106.4531, -108.7300, -115.4155,  ..., -119.6631, -119.1774,\n",
      "          -110.7877],\n",
      "         [-146.7139, -145.9828, -146.9487,  ..., -155.2113, -158.0557,\n",
      "          -139.4035]]], grad_fn=<UnsafeViewBackward0>)\n",
      "My model logits\n",
      "tensor([[[ -35.8260,  -35.1460,  -39.0735,  ...,  -42.4222,  -41.7547,\n",
      "           -35.9785],\n",
      "         [-107.7155, -108.0055, -113.2770,  ..., -116.4519, -115.7324,\n",
      "          -110.8482],\n",
      "         [-111.7447, -111.5662, -114.5354,  ..., -120.7213, -117.1712,\n",
      "          -112.3930],\n",
      "         [ -86.1794,  -88.5035,  -94.3420,  ..., -101.3490,  -98.6924,\n",
      "           -91.1553],\n",
      "         [-106.4438, -108.7262, -115.3992,  ..., -119.6509, -119.1691,\n",
      "          -110.7780],\n",
      "         [-146.6986, -145.9709, -146.9278,  ..., -155.1906, -158.0314,\n",
      "          -139.3904]]], grad_fn=<ViewBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Model logits\")\n",
    "print(\"GPT2 model logits\")\n",
    "gpt2_logits = gpt2_model(**tokenizer(text, return_tensors='pt')).logits\n",
    "print(gpt2_logits)\n",
    "print(\"My model logits\")\n",
    "my_logits = my_model(tokens)\n",
    "print(my_logits)\n",
    "\n",
    "print(t.allclose(gpt2_logits, my_logits, atol=1e-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argmax sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  464, 13601,  4196,    11,   416,  5564,  1940, 41471,    13,   220,\n",
       "           198, 26302,   379,  1755,    11]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The Large Apple, by Roald Dahl. \\nLate at night,\"\n",
    "tokens_gpt2 = tokenizer(text, return_tensors='pt')\n",
    "tokens_curr = tokenizer(text, return_tensors='pt')['input_ids']\n",
    "\n",
    "tokens_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    gpt2_next_token = gpt2_model(**tokens_gpt2).logits[0][-1].argmax()\n",
    "    my_model_next_token = my_model(tokens_curr)[0][-1].argmax()\n",
    "\n",
    "    tokens_gpt2['input_ids'] = t.cat([tokens_gpt2['input_ids'], gpt2_next_token.reshape((1,1))], dim=1)\n",
    "    tokens_gpt2['attention_mask'] = t.cat([tokens_gpt2['attention_mask'], t.tensor(1).reshape((1,1))], dim=1)\n",
    "    tokens_curr = t.cat([tokens_curr, my_model_next_token.reshape((1,1))], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 argmax output\n",
      "The Large Apple, by Roald Dahl. \n",
      "Late at night, I was sitting in the living room of my home\n",
      "\n",
      "My implementation output\n",
      "The Large Apple, by Roald Dahl. \n",
      "Late at night, I was sitting in the living room of my home\n"
     ]
    }
   ],
   "source": [
    "print(\"GPT2 argmax output\")\n",
    "print(''.join(tokenizer.convert_ids_to_tokens(tokens_gpt2['input_ids'][0])).replace('Ġ', ' ').replace('Ċ', '\\n'))\n",
    "print()\n",
    "print(\"My implementation output\")\n",
    "print(''.join(tokenizer.convert_ids_to_tokens(tokens_curr[0])).replace('Ġ', ' ').replace('Ċ', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal sampling from my version of the GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
       "         28000, 19942,  2877,   287,   257,  6569,    11,  4271, 31286,  1850,\n",
       "         19272,    11,   287,   262,   843,   274, 21124,    13,  3412,   517,\n",
       "          6452,   284,   262,  4837,   373,   262,  1109,   326,   262, 28000,\n",
       "         19942,  5158,  2818,  3594,    13]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://openai.com/index/better-language-models/\n",
    "text = \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\n",
    "tokens = tokenizer(text, return_tensors='pt')['input_ids']\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    my_model_next_token = t.multinomial(t.softmax(my_model(tokens)[0][-1], dim=0), 1)\n",
    "\n",
    "    tokens = t.cat([tokens, my_model_next_token.unsqueeze(0)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 completion of unicorn prompt\n",
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. This clearly opens the door to potential insights into human language.\n",
      "\n",
      "Warwicz had his humble beginnings in the fall of 1948 as a visiting missionary at Judeo-Christian monastery in Guatemala City, growing up in the tightly knit neighborhoods of Bolivian refugee camps, living close to his ancestral families, where his desire to learn had never been burned. Lacking a university education, he passed Jeremiah B. McCaw in segregation and Catholic school records, and was the youngest of three children. He\n"
     ]
    }
   ],
   "source": [
    "print(\"GPT2 completion of unicorn prompt\")\n",
    "print(''.join(tokenizer.convert_ids_to_tokens(tokens[0])).replace('Ġ', ' ').replace('Ċ', '\\n'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_dev",
   "language": "python",
   "name": "gen_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e1a9d7885dea6998a7a132fd9b87478e1e0c0674c7c69db4242a360dd50e282"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
