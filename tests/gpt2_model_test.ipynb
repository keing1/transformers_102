{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I validate my implementation of transformer components and GPT-2 small by:\n",
    "- Comparing the behavior of a GPT-2 transformer block with my own implementation of a GPT-2 transformer block\n",
    "- Comparing the behavior of the GPT-2 embedding components with my own implementation of the GPT-2 embedding components\n",
    "- Comparing the behavior of the full GPT-2 model with my own implementation of the full GPT-2 model\n",
    "\n",
    "There are slight deviations between the outputs of my model and that of the original GPT-2 model. As shown below, this appears to be primarily due to the original GPT-2 model using a slightly different variant of GeLU from both me and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import sys\n",
    "import os\n",
    "notebook_path = os.path.abspath('')\n",
    "project_root = os.path.join(notebook_path, '..')\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing GPT2 transformer block (and components) with my implementation of a GPT2 transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "TransformerDecoderBlock(\n",
      "  (norm_layer1): LayerNorm()\n",
      "  (norm_layer2): LayerNorm()\n",
      "  (mha_block): MultiheadAttentionBlock(\n",
      "    (linear_q): Linear()\n",
      "    (linear_k): Linear()\n",
      "    (linear_v): Linear()\n",
      "    (linear_o): Linear()\n",
      "  )\n",
      "  (mlp_block): MLPBlock(\n",
      "    (linear1): Linear()\n",
      "    (linear2): Linear()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from src.blocks import TransformerDecoderBlock\n",
    "\n",
    "gpt2_block = model.h[0]\n",
    "my_block = TransformerDecoderBlock(768, 12, 3072, 'mlpblock', 'gelu', 'layer_norm')\n",
    "\n",
    "my_block.norm_layer1.weight, my_block.norm_layer1.bias = gpt2_block.ln_1.weight, gpt2_block.ln_1.bias\n",
    "my_block.norm_layer2.weight, my_block.norm_layer2.bias = gpt2_block.ln_2.weight, gpt2_block.ln_2.bias\n",
    "\n",
    "my_block.mlp_block.linear1.weight, my_block.mlp_block.linear1.bias = t.nn.Parameter(gpt2_block.mlp.c_fc.weight.T), gpt2_block.mlp.c_fc.bias\n",
    "my_block.mlp_block.linear2.weight, my_block.mlp_block.linear2.bias = t.nn.Parameter(gpt2_block.mlp.c_proj.weight.T), gpt2_block.mlp.c_proj.bias\n",
    "\n",
    "(wq,wk,wv) = t.chunk(gpt2_block.attn.c_attn.weight.T, 3, dim=0)\n",
    "(bq,bk,bv) = t.chunk(gpt2_block.attn.c_attn.bias, 3, dim=0)\n",
    "\n",
    "my_block.mha_block.linear_q.weight, my_block.mha_block.linear_q.bias = t.nn.Parameter(wq), t.nn.Parameter(bq)\n",
    "my_block.mha_block.linear_k.weight, my_block.mha_block.linear_k.bias = t.nn.Parameter(wk), t.nn.Parameter(bk)\n",
    "my_block.mha_block.linear_v.weight, my_block.mha_block.linear_v.bias = t.nn.Parameter(wv), t.nn.Parameter(bv)\n",
    "my_block.mha_block.linear_o.weight, my_block.mha_block.linear_o.bias = t.nn.Parameter(gpt2_block.attn.c_proj.weight.T), gpt2_block.attn.c_proj.bias\n",
    "\n",
    "my_block.eval()\n",
    "gpt2_block.eval()\n",
    "\n",
    "print(gpt2_block)\n",
    "print(my_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing transformer blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 Transformer block\n",
      "tensor([[[11.8319,  8.1987,  4.1071,  ...,  7.8378, 22.8137,  2.5132],\n",
      "         [-0.4134, -8.7172,  0.4104,  ...,  8.0247,  7.2181, 14.3804],\n",
      "         [ 0.9540, -5.9592, -7.2451,  ...,  1.9031,  0.2349, 11.5673],\n",
      "         [ 8.3677,  4.3870, -2.9901,  ...,  3.9888, -5.4587,  3.0287],\n",
      "         [-1.1786, -0.9616,  2.8636,  ...,  3.2149, -2.4172,  2.9858]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "My transformer block\n",
      "tensor([[[11.8319,  8.1991,  4.1074,  ...,  7.8380, 22.8131,  2.5124],\n",
      "         [-0.4140, -8.7163,  0.4102,  ...,  8.0232,  7.2171, 14.3786],\n",
      "         [ 0.9541, -5.9614, -7.2454,  ...,  1.9033,  0.2347, 11.5671],\n",
      "         [ 8.3662,  4.3870, -2.9897,  ...,  3.9888, -5.4584,  3.0299],\n",
      "         [-1.1787, -0.9614,  2.8644,  ...,  3.2139, -2.4160,  2.9847]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = t.randn((1,5,768))\n",
    "\n",
    "seq_len = x.shape[-2]\n",
    "att_mask = t.where(t.arange(seq_len).unsqueeze(1) < t.arange(seq_len), -t.inf, 0)\n",
    "\n",
    "gpt2_block.eval()\n",
    "my_block.eval()\n",
    "\n",
    "print(\"GPT2 Transformer block\")\n",
    "print(gpt2_block(x)[0])\n",
    "print(\"My transformer block\")\n",
    "print(my_block(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization\n",
      "GPT2 Layer Norm 1\n",
      "tensor([[[ 0.1271,  0.0875,  0.3001,  ...,  0.0327,  0.3548, -0.0742],\n",
      "         [-0.1189, -0.4109,  0.0088,  ..., -0.0258, -0.0474,  0.0364],\n",
      "         [ 0.3369, -0.1295, -0.0517,  ..., -0.0753,  0.0250,  0.0757],\n",
      "         [-0.0990, -0.1408,  0.0405,  ...,  0.0439, -0.0470,  0.1307],\n",
      "         [ 0.0546,  0.1112, -0.1200,  ...,  0.1650, -0.2234,  0.1034]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "My Layer Norm 1\n",
      "tensor([[[ 0.1271,  0.0875,  0.3001,  ...,  0.0327,  0.3548, -0.0742],\n",
      "         [-0.1189, -0.4109,  0.0088,  ..., -0.0258, -0.0474,  0.0364],\n",
      "         [ 0.3369, -0.1295, -0.0517,  ..., -0.0753,  0.0250,  0.0757],\n",
      "         [-0.0990, -0.1408,  0.0405,  ...,  0.0439, -0.0470,  0.1307],\n",
      "         [ 0.0546,  0.1112, -0.1200,  ...,  0.1650, -0.2234,  0.1034]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "GPT2 Layer Norm 2\n",
      "tensor([[[ 0.1192,  0.1020,  0.4947,  ...,  0.2802,  2.6047, -0.3328],\n",
      "         [-0.0251, -0.4713,  0.1025,  ..., -0.1178, -0.2273,  0.2935],\n",
      "         [ 0.2423, -0.1476,  0.0212,  ..., -0.4548,  0.2826,  0.5158],\n",
      "         [-0.0134, -0.1607,  0.1453,  ...,  0.3565, -0.2246,  0.8269],\n",
      "         [ 0.0767,  0.1293, -0.0709,  ...,  1.1808, -1.4667,  0.6722]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "My Layer Norm 2\n",
      "tensor([[[ 0.1192,  0.1020,  0.4947,  ...,  0.2802,  2.6047, -0.3328],\n",
      "         [-0.0251, -0.4713,  0.1025,  ..., -0.1178, -0.2273,  0.2935],\n",
      "         [ 0.2423, -0.1476,  0.0212,  ..., -0.4548,  0.2826,  0.5158],\n",
      "         [-0.0134, -0.1607,  0.1453,  ...,  0.3565, -0.2246,  0.8269],\n",
      "         [ 0.0767,  0.1293, -0.0709,  ...,  1.1808, -1.4667,  0.6722]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalization\")\n",
    "\n",
    "print(\"GPT2 Layer Norm 1\")\n",
    "print(gpt2_block.ln_1(x))\n",
    "print(\"My Layer Norm 1\")\n",
    "print(my_block.norm_layer1(x))\n",
    "\n",
    "print(\"GPT2 Layer Norm 2\")\n",
    "print(gpt2_block.ln_2(x))\n",
    "print(\"My Layer Norm 2\")\n",
    "print(my_block.norm_layer2(x))\n",
    "\n",
    "print(t.allclose(my_block.norm_layer1(x), gpt2_block.ln_1(x), atol=1e-8))\n",
    "print(t.allclose(my_block.norm_layer2(x), gpt2_block.ln_2(x), atol=1e-8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head attention\n",
      "GPT2 Attention\n",
      "tensor([[[ 6.6055e-01, -7.5954e+00, -5.4793e+00,  ..., -3.6606e-01,\n",
      "           7.8667e-01,  1.6924e-01],\n",
      "         [-1.4892e-01, -8.9095e+00,  2.7064e-01,  ..., -1.6242e-01,\n",
      "           1.8249e-01, -5.3543e-01],\n",
      "         [-7.1339e+00, -2.3403e+01, -7.0618e+00,  ..., -3.9493e-01,\n",
      "           1.4001e+00,  8.8264e-01],\n",
      "         [ 4.2808e+00,  6.6949e+00,  6.5864e+00,  ..., -4.0145e-01,\n",
      "           1.1179e+00, -1.4970e-02],\n",
      "         [-6.9755e+00,  8.2094e-01, -3.4451e+00,  ..., -7.4120e-02,\n",
      "           5.3183e-02, -9.0514e-01]]], grad_fn=<ViewBackward0>)\n",
      "My attention\n",
      "tensor([[[ 6.6055e-01, -7.5954e+00, -5.4793e+00,  ..., -3.6606e-01,\n",
      "           7.8667e-01,  1.6924e-01],\n",
      "         [-1.4892e-01, -8.9095e+00,  2.7065e-01,  ..., -1.6242e-01,\n",
      "           1.8249e-01, -5.3543e-01],\n",
      "         [-7.1339e+00, -2.3403e+01, -7.0618e+00,  ..., -3.9493e-01,\n",
      "           1.4001e+00,  8.8264e-01],\n",
      "         [ 4.2808e+00,  6.6949e+00,  6.5864e+00,  ..., -4.0145e-01,\n",
      "           1.1179e+00, -1.4971e-02],\n",
      "         [-6.9755e+00,  8.2093e-01, -3.4451e+00,  ..., -7.4121e-02,\n",
      "           5.3183e-02, -9.0514e-01]]], grad_fn=<AsStridedBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Multi-head attention\")\n",
    "print(\"GPT2 Attention\")\n",
    "print(gpt2_block.attn(x)[0])\n",
    "print(\"My attention\")\n",
    "print(my_block.mha_block(x, attention_mask=att_mask))\n",
    "\n",
    "\n",
    "print(t.allclose(my_block.mha_block(x, attention_mask=att_mask), gpt2_block.attn(x)[0], atol=1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "GPT2 MLP\n",
      "tensor([[[ 17.9010,  13.8917,  14.8560,  ...,  12.2650,  28.4097,  11.8572],\n",
      "         [ -0.8464,  -1.3232,   0.1836,  ...,  12.6661,   2.7973,  12.5186],\n",
      "         [ -5.7418, -10.0784, -12.0590,  ...,  17.7406, -12.0171,  19.6903],\n",
      "         [ 15.4184,   2.8175, -12.2079,  ...,  13.9143,  -8.7852,   9.1022],\n",
      "         [  3.1879, -10.1602,   5.9635,  ...,   4.8981,   6.2863,   6.4367]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "My MLP\n",
      "tensor([[[ 17.9005,  13.8931,  14.8554,  ...,  12.2647,  28.4102,  11.8568],\n",
      "         [ -0.8461,  -1.3219,   0.1840,  ...,  12.6645,   2.7964,  12.5188],\n",
      "         [ -5.7424, -10.0783, -12.0588,  ...,  17.7390, -12.0182,  19.6907],\n",
      "         [ 15.4166,   2.8183, -12.2077,  ...,  13.9135,  -8.7857,   9.1014],\n",
      "         [  3.1873, -10.1606,   5.9645,  ...,   4.8993,   6.2862,   6.4379]]],\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"MLP\")\n",
    "print(\"GPT2 MLP\")\n",
    "print(gpt2_block.mlp(x))\n",
    "print(\"My MLP\")\n",
    "print(my_block.mlp_block(x))\n",
    "\n",
    "print(t.allclose(my_block.mlp_block(x), gpt2_block.mlp(x), atol=1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing GeLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeLU\n",
      "GPT2 GeLU\n",
      "tensor([-0.1350, -0.0685, -0.1540, -0.1424,  0.0410, -0.1226,  0.2493, -0.1480,\n",
      "        -0.1072,  0.6838])\n",
      "My GeLU\n",
      "tensor([-0.1350, -0.0685, -0.1539, -0.1422,  0.0410, -0.1226,  0.2493, -0.1480,\n",
      "        -0.1072,  0.6839])\n",
      "Torch GeLU\n",
      "tensor([-0.1350, -0.0685, -0.1539, -0.1422,  0.0410, -0.1226,  0.2493, -0.1480,\n",
      "        -0.1072,  0.6839])\n"
     ]
    }
   ],
   "source": [
    "from src.activations import gelu\n",
    "from torch.nn.functional import gelu as torch_gelu\n",
    "\n",
    "print(\"GeLU\")\n",
    "x = t.randn((10,))\n",
    "\n",
    "print(\"GPT2 GeLU\")\n",
    "print(gpt2_block.mlp.act(x))\n",
    "print(\"My GeLU\")\n",
    "print(gelu(x))\n",
    "print(\"Torch GeLU\")\n",
    "print(torch_gelu(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing MLP using GPT2 GeLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My MLP with their GELU\n",
      "GPT2 MLP Block\n",
      "tensor([[[ -2.9164,  -0.0618,  15.8952,  ...,   4.7298,  20.2631,   9.4219],\n",
      "         [  5.4619, -13.5041,  -4.1717,  ...,   8.0976,  12.3699,   6.2095],\n",
      "         [ -5.8160,  -0.1967, -12.2969,  ..., -16.4890,  -2.2318,  18.7375],\n",
      "         ...,\n",
      "         [ -7.1169,   2.0982,  -2.5297,  ...,   0.4998,  18.7223, -13.8197],\n",
      "         [ 27.7963, -26.1246, -13.4007,  ...,  15.4092,  15.5402,   7.5306],\n",
      "         [ -9.6909,  -1.7319,  -8.2100,  ...,   8.1588,   8.5199,  34.5271]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "My MLP Block with GPT2 GeLU\n",
      "tensor([[[ -2.9164,  -0.0618,  15.8952,  ...,   4.7298,  20.2631,   9.4219],\n",
      "         [  5.4619, -13.5041,  -4.1717,  ...,   8.0976,  12.3699,   6.2095],\n",
      "         [ -5.8160,  -0.1967, -12.2969,  ..., -16.4890,  -2.2318,  18.7375],\n",
      "         ...,\n",
      "         [ -7.1169,   2.0982,  -2.5297,  ...,   0.4998,  18.7223, -13.8197],\n",
      "         [ 27.7963, -26.1246, -13.4007,  ...,  15.4092,  15.5402,   7.5306],\n",
      "         [ -9.6909,  -1.7319,  -8.2100,  ...,   8.1588,   8.5199,  34.5271]]],\n",
      "       grad_fn=<AsStridedBackward0>)\n",
      "True\n",
      "The main difference between our models is in the MLP block, because OpenAI used a slightly different version of GeLU.\n"
     ]
    }
   ],
   "source": [
    "print(\"My MLP with their GELU\")\n",
    "x = t.randn((1, 10, 768))\n",
    "\n",
    "print(\"GPT2 MLP Block\")\n",
    "gpt2_mlp = gpt2_block.mlp(x)\n",
    "print(gpt2_mlp)\n",
    "\n",
    "print(\"My MLP Block with GPT2 GeLU\")\n",
    "my_mlp = my_block.mlp_block.linear2(gpt2_block.mlp.act(my_block.mlp_block.linear1(x)))\n",
    "print(my_mlp)\n",
    "\n",
    "print(t.allclose(gpt2_mlp, my_mlp, atol=1e-4))\n",
    "\n",
    "print(\"The main difference between our models is in the MLP block, because OpenAI used a slightly different version of GeLU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing GPT2 embedding with my implementation of the GPT2 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1212,  318,  257, 1332, 6827,   13]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import GPT2SmallModel\n",
    "my_model = GPT2SmallModel()\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "my_model.embedding_layer.position_embedding_layer.weight = model.wpe.weight\n",
    "my_model.embedding_layer.token_embedding_layer.weight = model.wte.weight\n",
    "\n",
    "text = \"This is a test sentence.\"\n",
    "tokens = t.tensor(tokenizer(text)['input_ids']).unsqueeze(0)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding outputs\n",
      "GPT2 embedding output\n",
      "tensor([[[ 0.0065, -0.2930,  0.0762,  ...,  0.0184, -0.0275,  0.1638],\n",
      "         [ 0.0142, -0.0437, -0.0393,  ...,  0.1487, -0.0278, -0.0255],\n",
      "         [-0.0464, -0.0791,  0.1016,  ...,  0.0623,  0.0928, -0.0598],\n",
      "         [-0.0580,  0.0095,  0.2207,  ..., -0.0635,  0.0760, -0.0543],\n",
      "         [-0.0888, -0.0326,  0.1666,  ..., -0.2539, -0.0370, -0.2046],\n",
      "         [ 0.0562, -0.0452,  0.1596,  ..., -0.0676,  0.0567,  0.0888]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "My embedding output\n",
      "tensor([[[ 0.0065, -0.2930,  0.0762,  ...,  0.0184, -0.0275,  0.1638],\n",
      "         [ 0.0142, -0.0437, -0.0393,  ...,  0.1487, -0.0278, -0.0255],\n",
      "         [-0.0464, -0.0791,  0.1016,  ...,  0.0623,  0.0928, -0.0598],\n",
      "         [-0.0580,  0.0095,  0.2207,  ..., -0.0635,  0.0760, -0.0543],\n",
      "         [-0.0888, -0.0326,  0.1666,  ..., -0.2539, -0.0370, -0.2046],\n",
      "         [ 0.0562, -0.0452,  0.1596,  ..., -0.0676,  0.0567,  0.0888]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding outputs\")\n",
    "print(\"GPT2 embedding output\")\n",
    "print(model.wte(tokens) + model.wpe(t.arange(tokens.shape[-1])))\n",
    "print(\"My embedding output\")\n",
    "print(my_model.embedding_layer(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing GPT2 model vs. my implementation of the GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1212,  318,  257, 1332, 6827,   13]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import GPT2SmallModel\n",
    "my_model = GPT2SmallModel()\n",
    "my_model.eval()\n",
    "\n",
    "# Importing GPT2LMHeadModel instead of GPT2Model because it applies the final unembed to get logits\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "gpt2_model.eval()\n",
    "\n",
    "text = \"This is a test sentence.\"\n",
    "tokens = t.tensor(tokenizer(text)['input_ids']).unsqueeze(0)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying over weights\n",
    "my_model.embedding_layer.position_embedding_layer.weight = gpt2_model.transformer.wpe.weight\n",
    "my_model.embedding_layer.token_embedding_layer.weight = gpt2_model.transformer.wte.weight\n",
    "\n",
    "my_model.final_ln.weight, my_model.final_ln.bias = gpt2_model.transformer.ln_f.weight, gpt2_model.transformer.ln_f.bias\n",
    "\n",
    "my_model_block_list = my_model.transformer_blocks\n",
    "gpt2_model_block_list = gpt2_model.transformer.h\n",
    "\n",
    "for my_block, gpt2_block in zip(my_model_block_list, gpt2_model_block_list):\n",
    "    my_block.norm_layer1.weight, my_block.norm_layer1.bias = gpt2_block.ln_1.weight, gpt2_block.ln_1.bias\n",
    "    my_block.norm_layer2.weight, my_block.norm_layer2.bias = gpt2_block.ln_2.weight, gpt2_block.ln_2.bias\n",
    "\n",
    "    my_block.mlp_block.linear1.weight, my_block.mlp_block.linear1.bias = t.nn.Parameter(gpt2_block.mlp.c_fc.weight.T), gpt2_block.mlp.c_fc.bias\n",
    "    my_block.mlp_block.linear2.weight, my_block.mlp_block.linear2.bias = t.nn.Parameter(gpt2_block.mlp.c_proj.weight.T), gpt2_block.mlp.c_proj.bias\n",
    "\n",
    "    (wq,wk,wv) = t.chunk(gpt2_block.attn.c_attn.weight.T, 3, dim=0)\n",
    "    (bq,bk,bv) = t.chunk(gpt2_block.attn.c_attn.bias, 3, dim=0)\n",
    "\n",
    "    my_block.mha_block.linear_q.weight, my_block.mha_block.linear_q.bias = t.nn.Parameter(wq), t.nn.Parameter(bq)\n",
    "    my_block.mha_block.linear_k.weight, my_block.mha_block.linear_k.bias = t.nn.Parameter(wk), t.nn.Parameter(bk)\n",
    "    my_block.mha_block.linear_v.weight, my_block.mha_block.linear_v.bias = t.nn.Parameter(wv), t.nn.Parameter(bv)\n",
    "    my_block.mha_block.linear_o.weight, my_block.mha_block.linear_o.bias = t.nn.Parameter(gpt2_block.attn.c_proj.weight.T), gpt2_block.attn.c_proj.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logits\n",
      "GPT2 model logits\n",
      "tensor([[[ -35.8890,  -35.2049,  -39.1336,  ...,  -42.4869,  -41.8197,\n",
      "           -36.0383],\n",
      "         [-107.7291, -108.0175, -113.2967,  ..., -116.4645, -115.7443,\n",
      "          -110.8654],\n",
      "         [-111.7507, -111.5704, -114.5443,  ..., -120.7242, -117.1756,\n",
      "          -112.3996],\n",
      "         [ -86.1846,  -88.5057,  -94.3530,  ..., -101.3573,  -98.6974,\n",
      "           -91.1616],\n",
      "         [-106.4531, -108.7300, -115.4155,  ..., -119.6631, -119.1774,\n",
      "          -110.7877],\n",
      "         [-146.7139, -145.9828, -146.9487,  ..., -155.2113, -158.0557,\n",
      "          -139.4035]]], grad_fn=<UnsafeViewBackward0>)\n",
      "My model logits\n",
      "tensor([[[ -35.8260,  -35.1460,  -39.0735,  ...,  -42.4222,  -41.7547,\n",
      "           -35.9785],\n",
      "         [-107.7155, -108.0055, -113.2770,  ..., -116.4519, -115.7324,\n",
      "          -110.8482],\n",
      "         [-111.7447, -111.5662, -114.5354,  ..., -120.7213, -117.1712,\n",
      "          -112.3930],\n",
      "         [ -86.1794,  -88.5035,  -94.3420,  ..., -101.3490,  -98.6924,\n",
      "           -91.1553],\n",
      "         [-106.4438, -108.7262, -115.3992,  ..., -119.6509, -119.1691,\n",
      "          -110.7780],\n",
      "         [-146.6986, -145.9709, -146.9278,  ..., -155.1906, -158.0314,\n",
      "          -139.3904]]], grad_fn=<ViewBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Model logits\")\n",
    "print(\"GPT2 model logits\")\n",
    "gpt2_logits = gpt2_model(**tokenizer(text, return_tensors='pt')).logits\n",
    "print(gpt2_logits)\n",
    "print(\"My model logits\")\n",
    "my_logits = my_model(tokens)\n",
    "print(my_logits)\n",
    "\n",
    "print(t.allclose(gpt2_logits, my_logits, atol=1e-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argmax sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  464, 13601,  4196,    11,   416,  5564,  1940, 41471,    13,   220,\n",
       "           198, 26302,   379,  1755,    11]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The Large Apple, by Roald Dahl. \\nLate at night,\"\n",
    "tokens_gpt2 = tokenizer(text, return_tensors='pt')\n",
    "tokens_curr = tokenizer(text, return_tensors='pt')['input_ids']\n",
    "\n",
    "tokens_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    gpt2_next_token = gpt2_model(**tokens_gpt2).logits[0][-1].argmax()\n",
    "    my_model_next_token = my_model(tokens_curr)[0][-1].argmax()\n",
    "\n",
    "    tokens_gpt2['input_ids'] = t.cat([tokens_gpt2['input_ids'], gpt2_next_token.reshape((1,1))], dim=1)\n",
    "    tokens_gpt2['attention_mask'] = t.cat([tokens_gpt2['attention_mask'], t.tensor(1).reshape((1,1))], dim=1)\n",
    "    tokens_curr = t.cat([tokens_curr, my_model_next_token.reshape((1,1))], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 argmax output\n",
      "The Large Apple, by Roald Dahl. \n",
      "Late at night, I was sitting in the living room of my home\n",
      "\n",
      "My implementation output\n",
      "The Large Apple, by Roald Dahl. \n",
      "Late at night, I was sitting in the living room of my home\n"
     ]
    }
   ],
   "source": [
    "print(\"GPT2 argmax output\")\n",
    "print(''.join(tokenizer.convert_ids_to_tokens(tokens_gpt2['input_ids'][0])).replace('Ġ', ' ').replace('Ċ', '\\n'))\n",
    "print()\n",
    "print(\"My implementation output\")\n",
    "print(''.join(tokenizer.convert_ids_to_tokens(tokens_curr[0])).replace('Ġ', ' ').replace('Ċ', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_dev",
   "language": "python",
   "name": "gen_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e1a9d7885dea6998a7a132fd9b87478e1e0c0674c7c69db4242a360dd50e282"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
